
> [!quote] **Hitopadesha** (Niti Shastra, India, ~12th Century)  
> **â€œà¤‰à¤¦à¥à¤¯à¥‹à¤—à¤¿à¤¨à¤‚ à¤ªà¥à¤°à¥à¤·à¤¸à¤¿à¤‚à¤¹à¤®à¥à¤ªà¥ˆà¤¤à¤¿ à¤²à¤•à¥à¤·à¥à¤®à¥€à¤ƒà¥¤â€**  
> **Pronunciation:** _Udyoginam puruá¹£a-siá¹ƒham upaiti laká¹£mÄ«á¸¥_  
> _â€œProsperity approaches the one who strives like a lion.â€_

# **RAG FOUNDATIONS : A Ocean's Snapshot**

> The purpose of this Foundations module is to build a strong conceptual baseline before writing code or constructing systems. Think of this section as the high-altitude map. We will explore deep internals and production workflows inside the Applied Practice module.

---

## **[[1.1 Definitions â€“ brief overview of essential terms and concepts]]**

- **RAG (Retrieval-Augmented Generation)** â€“ combining external knowledge retrieval with LLM reasoning
    
- **Retrieval** â€“ searching a knowledge base for relevant text to answer a query
    
- **Embeddings** â€“ mathematical vector representation of text that captures semantic similarity
    
- **Vector Database / Vector Store** â€“ database optimized for similarity search over embeddings (FAISS / Milvus / Pinecone / Qdrant)
    
- **Chunking** â€“ splitting long documents into smaller units for accurate retrieval
    
- **Similarity Search** â€“ finding top-k closest vectors using cosine / dot product / Euclidean distance
    
- **Reranking** â€“ re-scoring candidate results to refine relevance
    
- **Context Window** â€“ maximum token capacity the LLM can process at once
    
- **Grounding** â€“ injecting real factual information to minimize hallucination
    
- **Hallucination** â€“ incorrect output confidently produced by the model without real evidence
    

---

## **[[1.2 Core Principles â€“ guiding rules, fundamentals, and theoretical base]]**

- **Externalized knowledge** â€“ LLM does reasoning, vector DB holds facts
    
- **Retrieve, then generate** â€“ search first, answer after
    
- **Precision over quantity** â€“ fewer relevant chunks > many noisy chunks
    
- **Embeddings quality determines accuracy** â€“ better vector representation â†’ higher relevance
    
- **Chunking strategy matters** â€“ structured segmentation improves search quality
    
- **Reranking reduces nonsense** â€“ filters out retrieved but irrelevant text
    
- **Optimize system-wide, not piecewise** â€“ retrieval + prompting + generation = single pipeline
    
- **Observability is critical** â€“ track retrieval hit-rate, latency, hallucination rate
    

---

## **[[1.3 Mental Models â€“ intuitive ways to understand how the system works]]**

- **â€œLLM = Thinker, Vector DB = Memoryâ€** â€“ reasoning separate from stored knowledge
    
- **â€œRAG is Search + Summarizeâ€** â€“ retrieve docs â†’ synthesize answer
    
- **â€œChunk = Page, Document = Bookâ€** â€“ small pages are easier to search
    
- **â€œReranker = Editorâ€** â€“ decides what information actually matters
    
- **â€œPrompt = Stage Setupâ€** â€“ retrieved context sets the scene for the LLM
    

---

## **[[1.4 Architecture Overview â€“ structural components and their interactions]]**

### **1.4.1 High-Level Diagram â€“ visual summary of the system**

```
User Query
      â†“
Query Embeddings
      â†“
Similarity Search â†’ Vector DB
      â†“
Top-K Relevant Chunks â†’ Reranker (optional)
      â†“
Context-Injected Prompt â†’ LLM
      â†“
Generated Answer
```

### **[[1.4.2 Components & Responsibilities â€“ what each part does]]**

- **Ingestion Pipeline** â€“ clean, segment, embed and index documents
    
- **Embedding Model** â€“ converts text to vectors capturing semantic meaning
    
- **Vector Store** â€“ efficient nearest-neighbor search of embeddings
    
- **Retriever** â€“ fetches the best matching chunks
    
- **Reranker** â€“ improves precision by deeper scoring
    
- **Prompt Builder** â€“ constructs final LLM prompt with context
    
- **LLM Generator** â€“ synthesizes answer using retrieved knowledge
    
- **Telemetry / Monitoring** â€“ inspect pipeline quality & failures
    

### **[[1.4.3 Data Flow â€“ how information moves through the system]]**

- **Raw Document** â†’ clean â†’ **chunk** â†’ embed â†’ **store**
    
- **User query** â†’ embed â†’ **retrieve top-k** â†’ rerank â†’ **build prompt** â†’ LLM output
    
- **Optional feedback loop** â†’ caching & refinement
    

---

## **[[1.5 Internals & Mechanics â€“ behind-the-scenes processes and algorithms]]**

- **Vector embedding encoding** â€“ text â†’ numeric vector (e.g. 768â€“3072 dimensions)
    
- **ANN (Approximate Nearest Neighbor) search** â€“ HNSW / IVF-PQ / Flat
    
- **Similarity metrics** â€“ cosine / dot product / L2 distance
    
- **Cross-encoder reranking** â€“ pairwise scoring for accuracy
    
- **Structured prompt injection** â€“ sectioned context, citation formatting
    
- **Context compression** â€“ summarization before LLM input
    
- **Caching & hybrid search** â€“ speed optimization for production scale
    

---

## **[[1.6 Limitations & Trade-offs â€“ what it cannot do and constraints to consider]]**

- **Context window limits** restrict how much data can be inserted
    
- **Latency increases** as data size & search complexity grow
    
- **Poor chunking leads to irrelevant retrieval**
    
- **Bad embeddings â†’ incorrect context selection**
    
- **Hallucinations still possible without tight grounding**
    
- **Reranking increases accuracy but adds cost & time**
    
- **Scalability challenges** with massive datasets
    

---

---

## **Choose what to generate next:**

a) **Basic RAG Code Example** (end-to-end minimal)  
b) **Detailed Architecture Diagram** (clean + labelled)  
c) **Cheatsheet for chunking & embeddings**  
d) **Industry-grade patterns & workflows**  
e) **Flashcards for Section 1**

**Reply with: a / b / c / d / e / all** ğŸš€